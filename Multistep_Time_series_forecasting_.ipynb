{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66240275",
   "metadata": {},
   "source": [
    "#  Multistep Timeseries LSTM Forecasting - Single Variable - Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba51c9",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e1512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from keras.layers import Dropout\n",
    "from keras.metrics import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e817e",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e003c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/aparuchuri/Desktop/AI_Ml/fabric-aiml/fabric_stats.csv\"\n",
    "filtered_path ='/Users/aparuchuri/Desktop/AI_Ml/fabric-aiml/filtered_data1.csv'\n",
    "# configure\n",
    "n_lag = 30 # Number of observations to look into past per prediction \n",
    "n_seq = 4500 # Number of points to predict into future [60,180,540,1620,4860]\n",
    "n_test = 1 # Number of test samples \n",
    "n_epochs = 500  #Number of iterations of training \n",
    "n_batch = 1 # Dont Change , Feed entire batch \n",
    "n_neurons = 64 # number of learning units per layer (capacity per layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfcf9d",
   "metadata": {},
   "source": [
    "# Developing ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345928c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.) convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "        agg = concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "        return agg\n",
    "\n",
    "#b.) create a differenced series\n",
    "\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    " \n",
    "#c.) transform series into train and test sets for supervised learning\n",
    "\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test\n",
    " \n",
    "#d.) Define the LSTM network \n",
    "\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True, return_sequences=True))\n",
    "    model.add(Dropout(0.2))  # Add dropout with a dropout rate of 0.2\n",
    "    model.add(LSTM(n_neurons, return_sequences=True))\n",
    "    model.add(Dropout(0.2))  # Add dropout with a dropout rate of 0.2\n",
    "    model.add(LSTM(n_neurons, return_sequences=True))\n",
    "    model.add(Dropout(0.2))  # Add dropout with a dropout rate of 0.2\n",
    "    model.add(LSTM(n_neurons))\n",
    "    model.add(Dropout(0.2))  # Add dropout with a dropout rate of 0.2\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[MeanAbsoluteError()])\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    " \n",
    "#e.) forecast with an LSTM,\n",
    "\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    " \n",
    "#f.)  evaluate the persistence model\n",
    "\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    " \n",
    "#g.) invert differenced forecast- reverse the stationary form\n",
    "\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    " \n",
    "#h.) inverse data transform on forecasts - reverse the scaled transformation applied earlier on data\n",
    "\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    " \n",
    "#i.) evaluate the model with RMSE \n",
    "\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    all_rmse = []\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        all_rmse.append(rmse)\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "    return all_rmse\n",
    " \n",
    "#j.) plot the forecasts\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    pyplot.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + forecasts[i]\n",
    "        pyplot.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3d1e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#Load and clean dataset\n",
    "\n",
    "sampling_time = 60\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(path)\n",
    "df['time'] = pd.to_datetime(df['time'], unit='ns')\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "pfe_spec = 'FPC0:PIC0:NPU0:DP0' # can specify other FPCs/PICs\n",
    "rate_bps_lower_limit = 0\n",
    "rate_bps_upper_limit = 1.8e12\n",
    "\n",
    "\n",
    "# Step 1: Drop First 1000 elements\n",
    "df = df[1000:]\n",
    "\n",
    "# Filter for one FPC: FPC0:PIC0:NPU0:DP0\n",
    "df_per_fpc_per_pfe = df[df['fru']== pfe_spec]\n",
    "\n",
    "# Filter for in-rate-bps >0 and out-rate-bps >0\n",
    "\n",
    "df_filtered = df_per_fpc_per_pfe[(df_per_fpc_per_pfe['out-rate-bps'] > rate_bps_lower_limit) & (df_per_fpc_per_pfe['out-rate-bps'] < rate_bps_upper_limit)]\n",
    "\n",
    "df_filtered = df_filtered[3200:] #removing all points before and including straight line in the dataset\n",
    "df_filtered['out-rate-bps'] = df_filtered['out-rate-bps']/1e12\n",
    "df_filtered.to_csv(filtered_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e805c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      out-rate-bps\n",
      "0         0.474935\n",
      "1         0.474480\n",
      "2         0.474978\n",
      "3         0.474912\n",
      "4         0.474406\n",
      "...            ...\n",
      "3718      0.494484\n",
      "3719      0.844184\n",
      "3720      0.633698\n",
      "3721      0.633637\n",
      "3722      0.633774\n",
      "\n",
      "[3723 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "series = read_csv(filtered_path, usecols=[8], engine='python')\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691/3691 [==============================] - 8s 1ms/step - loss: 0.0627 - mean_absolute_error: 0.1395\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0547 - mean_absolute_error: 0.1424\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0530 - mean_absolute_error: 0.1422\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0513 - mean_absolute_error: 0.1407\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0493 - mean_absolute_error: 0.1381\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0464 - mean_absolute_error: 0.1324\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0455 - mean_absolute_error: 0.1293\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0438 - mean_absolute_error: 0.1277\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0422 - mean_absolute_error: 0.1241\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0416 - mean_absolute_error: 0.1210\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0393 - mean_absolute_error: 0.1162\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0381 - mean_absolute_error: 0.1138\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0350 - mean_absolute_error: 0.1080\n",
      "3691/3691 [==============================] - 6s 2ms/step - loss: 0.0345 - mean_absolute_error: 0.1052\n",
      "3691/3691 [==============================] - 6s 2ms/step - loss: 0.0333 - mean_absolute_error: 0.1003\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0324 - mean_absolute_error: 0.0988\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0301 - mean_absolute_error: 0.0942\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0295 - mean_absolute_error: 0.0908\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0282 - mean_absolute_error: 0.0889\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0284 - mean_absolute_error: 0.0892\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0278 - mean_absolute_error: 0.0888\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0272 - mean_absolute_error: 0.0870\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0267 - mean_absolute_error: 0.0859\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0249 - mean_absolute_error: 0.0817\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0245 - mean_absolute_error: 0.0817\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.0803\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0235 - mean_absolute_error: 0.0799\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0232 - mean_absolute_error: 0.0768\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0229 - mean_absolute_error: 0.0781\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0233 - mean_absolute_error: 0.0772\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0222 - mean_absolute_error: 0.0757\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0223 - mean_absolute_error: 0.0769\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0208 - mean_absolute_error: 0.0728\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0214 - mean_absolute_error: 0.0760\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0215 - mean_absolute_error: 0.0746\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0210 - mean_absolute_error: 0.0743\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0204 - mean_absolute_error: 0.0714\n",
      "3691/3691 [==============================] - 6s 2ms/step - loss: 0.0205 - mean_absolute_error: 0.0728\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0198 - mean_absolute_error: 0.0712\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0212 - mean_absolute_error: 0.0735\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0199 - mean_absolute_error: 0.0716\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0189 - mean_absolute_error: 0.0698\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0191 - mean_absolute_error: 0.0713\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0197 - mean_absolute_error: 0.0714\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0183 - mean_absolute_error: 0.0684\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0186 - mean_absolute_error: 0.0700\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0191 - mean_absolute_error: 0.0678\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0180 - mean_absolute_error: 0.0676\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0191 - mean_absolute_error: 0.0707\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0177 - mean_absolute_error: 0.0677\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0176 - mean_absolute_error: 0.0686\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0178 - mean_absolute_error: 0.0673\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0179 - mean_absolute_error: 0.0670\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0172 - mean_absolute_error: 0.0666\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0174 - mean_absolute_error: 0.0665\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0173 - mean_absolute_error: 0.0652\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0162 - mean_absolute_error: 0.0633\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0162 - mean_absolute_error: 0.0640\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0164 - mean_absolute_error: 0.0646\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0165 - mean_absolute_error: 0.0643\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0156 - mean_absolute_error: 0.0630\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0156 - mean_absolute_error: 0.0628\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0162 - mean_absolute_error: 0.0633\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0154 - mean_absolute_error: 0.0608\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0156 - mean_absolute_error: 0.0622\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0155 - mean_absolute_error: 0.0613\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0152 - mean_absolute_error: 0.0599\n",
      "3691/3691 [==============================] - 5s 1ms/step - loss: 0.0154 - mean_absolute_error: 0.0609\n",
      "3691/3691 [==============================] - 6s 2ms/step - loss: 0.0148 - mean_absolute_error: 0.0604\n",
      " 548/3691 [===>..........................] - ETA: 4s - loss: 0.0167 - mean_absolute_error: 0.0574"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "#fit model\n",
    "start_time = time.time()\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "#forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "#inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "\n",
    "#evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6fb126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot forecasts\n",
    "n_test = 1\n",
    "plot_forecasts(series, forecasts, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c29fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model fit on the train dataset\n",
    "train_forecasts = make_forecasts(model, n_batch, train, train, n_lag, n_seq)\n",
    "train_forecasts = inverse_transform(series, train_forecasts, scaler, len(train) + n_seq - n_lag - 1)\n",
    "train_actual = [row[n_lag:] for row in train]\n",
    "train_actual = inverse_transform(series, train_actual, scaler, len(train) + n_seq - n_lag - 1)\n",
    "\n",
    "# Evaluate and print RMSE for train dataset\n",
    "# print(\"Train Dataset Evaluation:\")\n",
    "# evaluate_forecasts(train_actual, train_forecasts, n_lag, n_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train forecasts\n",
    "n_test = 1\n",
    "plot_forecasts(series, train_forecasts, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91537b61",
   "metadata": {},
   "source": [
    "## Theory for Hyper Parameters:\n",
    "\n",
    "Changing each of the configuration parameters will have different effects on the LSTM model and its predictions. Here's an explanation of what each parameter controls:\n",
    "\n",
    "n_lag: This parameter defines the number of lag observations (input features) to use for predicting the next time step. Increasing n_lag allows the model to consider more past observations, potentially capturing more complex patterns. However, it also increases the dimensionality of the input data and may require more training data to learn effectively.\n",
    "\n",
    "n_seq: The n_seq parameter determines the number of forecasted observations (output labels) to predict. Increasing n_seq extends the forecast horizon, allowing the model to make predictions further into the future. However, longer forecast horizons are generally more challenging, and the accuracy of the model's predictions may decrease.\n",
    "\n",
    "n_test: The n_test parameter specifies the number of samples to reserve for testing the model's performance. It determines how many time steps from the end of the dataset will be used for evaluation. Changing n_test allows you to control the size of the test set and assess the model's accuracy on a specific timeframe.\n",
    "\n",
    "n_epochs: This parameter defines the number of training epochs, which is the number of times the entire training dataset is passed through the LSTM model during training. Increasing n_epochs allows the model to train for a longer duration, potentially improving its accuracy. However, setting n_epochs too high can lead to overfitting if the model starts memorizing the training data without generalizing well to unseen data.\n",
    "\n",
    "n_batch: The n_batch parameter determines the batch size, which is the number of samples used in each mini-batch during training. Changing n_batch affects the speed and stability of the training process. Larger batch sizes can lead to faster training, but they may also require more memory. Smaller batch sizes provide more frequent updates to the model's weights but may result in slower training progress.\n",
    "\n",
    "n_neurons: The n_neurons parameter controls the number of neurons (units) in the LSTM layer. Increasing n_neurons can enable the model to capture more complex patterns in the data but also increases the model's capacity and the number of trainable parameters. Too many neurons may lead to overfitting, while too few neurons may limit the model's ability to learn complex relationships.\n",
    "By adjusting these configuration parameters, you can experiment with different settings to find the optimal values that balance model complexity, training time, and predictive accuracy for your specific dataset and task. It often requires some trial and error to determine the best combination of these parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
